{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","source":["![picture](https://prowly-uploads.s3.eu-west-1.amazonaws.com/uploads/4626/assets/71776/large_logo_wsb_poziom.jpg)"],"metadata":{"id":"tAWHQgdQZIzJ"}},{"cell_type":"markdown","metadata":{"id":"EtsagzVvWdDf"},"source":["# WPROWADZENIE DO ALGORYTMÓW GŁĘBOKIEGO UCZENIA MASZYNOWEGO - DEEP LEARNING PYTORCH\n","\n","## Wprowadzenie do sieci neuronowych w pythonie (pytorch):\n","<ul>\n","    <li>Model sequential</li>\n","    <li>nn.Module</li>\n","    <li>Jak uczyć model pytorch?</li>\n","    <li>Dataloader</li>\n","    <li>Case study</li>\n","    <li>Tensorboard - wybór najlepszych parametrów sieci</li>\n","    <li>XAI</li>\n","    </li>\n","</ul>"]},{"cell_type":"markdown","source":["# Biblioteki"],"metadata":{"id":"aAQf34Z_AIcG"}},{"cell_type":"code","metadata":{"id":"6xp8JRK3WdDg"},"source":["import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import plotly.graph_objects as go"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import copy"],"metadata":{"id":"RNO-v-EH4dpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LAhKFsi1WdDh"},"source":["# Zdefiniujmy proste zbiore danych, których użyjemy w procesie uczenia się sieci neuronowych\n","zbiór_liniowy = np.zeros([100,2])\n","zbiór_liniowy[:,0] = [x/100 for x in range(100)]\n","zbiór_liniowy[:,1] = [2*x+3 for x in zbiór_liniowy[:,0]]\n","plt.plot(zbiór_liniowy[:,0],zbiór_liniowy[:,1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIi-rMR6WdDy"},"source":["XOR = np.zeros([400,4])\n","XOR[:100,1:3] = np.array([np.random.rand(100)/5,\n","                           np.random.rand(100)/5]).reshape(100,2)\n","XOR[:100,2] = XOR[:100,2]+1                      \n","XOR[100:200,1:] = np.array([1+np.random.rand(100)/3,\n","                            1+np.random.rand(100)/2,\n","                            1+np.random.rand(100)/10]).reshape(100,3)\n","XOR[200:300,[0,2,3]] = np.array([1+np.random.rand(100)/10,\n","                            1+np.random.rand(100)/2,\n","                            1+np.random.rand(100)/3]).reshape(100,3)\n","XOR[300:,:2] = np.array([1+np.random.rand(100)/3,\n","                            1+np.random.rand(100)/5]).reshape(100,2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Wprowadzenie do sieci neuronowych w pytorch"],"metadata":{"id":"M9POBMUXvf84"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"],"metadata":{"id":"0Cw1MuPavjUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["W pytorch trochę inaczej korzystamy z GPU - na potrzeby dzisiejszych zajęć skupimy się na CPU, w modelach CNN pokażemy sobie model wytrenowany na GPU."],"metadata":{"id":"45CxYVDmY1-4"}},{"cell_type":"code","source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"],"metadata":{"id":"cJUdx61uvjOr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model sequential"],"metadata":{"id":"1khUgvjXHeFO"}},{"cell_type":"markdown","source":["https://pytorch.org/docs/stable/nn.html#\n","\n","https://pytorch.org/docs/stable/nn.functional.html"],"metadata":{"id":"FIqoxvkgMEHf"}},{"cell_type":"code","source":["model = nn.Sequential( # podobnie jak w tensorflow\n","    nn.Linear(\n","      1, # wymiarowosc zbioru\n","      10),# liczba neuronow\n","    nn.ReLU(), # Tutaj ReLU nie jest traktowana jako warstwa, tylko jak funkcja aktywacji\n","    nn.Linear(10, 1)# Zauważ, że w pytorch podajemy wymiarowość zbioru wejściowego i wyjściowego!\n","    )\n","print(model)"],"metadata":{"id":"MsmYtOfnHgX9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_function = nn.MSELoss() # Definicja funkcji kosztu\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001) # definicja optimizera"],"metadata":{"id":"VsO7euLlHgV1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Podstawowa pętla treningowa"],"metadata":{"id":"HlxnS94hzAZJ"}},{"cell_type":"markdown","source":["Pytorch uczy się na tensorach - więc wektory musimy zamienić na typ \"torch.tensor\". Dodatkowo pamiętajcie o castowaniu wektorów do odpowiedniego urządenia (device). Model i wektor muszą korzystać z tego samego akceleratora obliczeniowego."],"metadata":{"id":"kwVcFIIia_2b"}},{"cell_type":"code","source":["x = torch.tensor(zbiór_liniowy[:,0], dtype=torch.float32).reshape(-1, 1).to(device) # przykładowe wektory wejściowy\n","y = torch.tensor(zbiór_liniowy[:,1], dtype=torch.float32).reshape(-1, 1).to(device) # przykładowe wektory wyjściowe"],"metadata":{"id":"ryfxOE13iLcy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prosta pętla treningowa\n","losses = []\n","for epoch in range(5000):\n","    pred_y = model(x) # predykcja\n","    loss = loss_function(pred_y, y) # obliczamy wartość funkcji kosztu\n","    losses.append(loss.item()) # dodajemy błąd do historii\n","\n","    model.zero_grad() # resetujemy gradient modelu\n","    loss.backward() # krok uczenia - wysyłamy informację o błędzie do modelu\n","\n","    optimizer.step() # update wag na podstawie obliczonego gradientu (który wyliczamy na podstawie funkcji kosztu)"],"metadata":{"id":"wSzRqgamHgTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(losses)\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"SNHOyDp6HgRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model(x)\n","predictions = predictions.detach().numpy().reshape(100) # Predykcje w numpy"],"metadata":{"id":"QKPqjGatj1A1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Zdefiniujmy proste zbiore danych, których użyjemy w procesie uczenia się sieci neuronowych\n","\n","plt.plot(zbiór_liniowy[:,0],zbiór_liniowy[:,1])\n","plt.plot(zbiór_liniowy[:,0],predictions)\n","plt.show()"],"metadata":{"id":"YytvwibNjcFC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Wykorzystanie nn.Module"],"metadata":{"id":"x73U8r2nHhhI"}},{"cell_type":"markdown","source":["W pytorch najłatwiej wykorzystać moduł nn, którego składnia jest bardzo zbliżona do tensorflow. nn.Sequential buduje graf obliczeniowy na podstawie kolejnych funkcji."],"metadata":{"id":"_x6QfJylx5jH"}},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","  def __init__(self, n_dims, n_class):\n","      super(NeuralNetwork, self).__init__()\n","      self.seq = nn.Sequential(\n","          nn.Linear(n_dims, 512),\n","          nn.ReLU(),\n","          nn.Linear(512, 512),\n","          nn.ReLU(),\n","          nn.Linear(512, n_class),\n","          nn.Sigmoid()\n","      )\n","\n","  def forward(self, x):\n","      logits = self.seq(x)\n","      return logits"],"metadata":{"id":"JBxf97GkvjIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = torch.rand(1, 3, device=device)"],"metadata":{"id":"rbj6GpKeCgRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = XOR[:,:3]\n","y = XOR[:,3].reshape([400,1])"],"metadata":{"id":"r_W0VAGdMR0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = (y>0)*1"],"metadata":{"id":"aCwCUxF9WXbL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = NeuralNetwork(3, 1)\n","print(model)"],"metadata":{"id":"MRVwpxNPv1HE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits = model(X)\n","print(f\"Predicted probabilites : {logits}\")\n","# Dla więcej niż 1 klasy:\n","y_pred = logits.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"],"metadata":{"id":"Ev_4zF4av1Em"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"XqB_jYlFNaR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# podzielmy zbior na testowy i treningowy\n","X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True)\n","X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n","y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n","X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n","y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)"],"metadata":{"id":"bbid_I9Gv1B0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 1e-2"],"metadata":{"id":"BFS2ZaEbvjF_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BCE - binary crossentropy, jest też wersja nn.BCEWITHLOGITSLOSS, która ma wbudowaną warstwę sigmoid\n","# W przypadku użycia BCEWITHLOGITSLOSS należy jako funkcję aktywacji ostatniej warstwy zostawić funkcję liniową\n","loss_fn = nn.BCELoss()"],"metadata":{"id":"uC1u8LQYxNMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"metadata":{"id":"FS_4rGMixNJi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pętla treningowa z early stopping oraz zapamiętywaniem najlepszych wag"],"metadata":{"id":"-wWQ8pbwzEjZ"}},{"cell_type":"code","source":["n_epochs = 100   # liczba iteracji\n","batch_size = 50  # rozmiar wsadu\n","batch_start = torch.arange(0, len(X_train), batch_size)\n"," \n","# parametry, aby zapamietać najlepszą iterację\n","best_loss = np.inf   # init to infinity\n","best_weights = None\n","early_stop_thresh = 10\n","history = []"],"metadata":{"id":"I_ENoTWsFGNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Zapisywanie modelu do pliku i jego wczytywanie\n","def checkpoint(model, filename):\n","    torch.save(model.state_dict(), filename)\n","def resume(model, filename):\n","    model.load_state_dict(torch.load(filename))"],"metadata":{"id":"gZlS1MUR1Em3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"38WYpKDf3ARb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# jak uczymy model pytorch\n","for epoch in range(n_epochs):\n","    model.train() # wprowadzamy model w tryb treningu\n","    with tqdm(batch_start, unit=\"batch\", mininterval=0) as bar: # tworzy pasek uczenia - wygodna sprawa do monitorowania\n","        bar.set_description(f\"Epoch {epoch}\")\n","        for start in bar:\n","            # wybierz obserwacje z poszczególnych batchów - zwróć uwagę, że w tym przypadku obserwacje nie są losowane\n","            X_batch = X_train[start:start+batch_size]\n","            y_batch = y_train[start:start+batch_size]\n","            # krok wprzód\n","            y_pred = model(X_batch)\n","            loss = loss_fn(y_pred, y_batch)\n","            # krok wstecz\n","            optimizer.zero_grad()\n","            loss.backward()\n","            # zmiana wartości wag\n","            optimizer.step()\n","            bar.set_postfix(loss = float(loss))\n","    # walidacja\n","    model.eval()\n","    y_pred = model(X_test)\n","    epoch_loss = loss_fn(y_pred, y_test)\n","    epoch_loss = float(epoch_loss)\n","    history.append(epoch_loss)\n","    if epoch_loss < best_loss:\n","        # spełnia funkcję modułu EarlyStopping\n","        best_loss = epoch_loss\n","        best_epoch = epoch\n","        best_weights = copy.deepcopy(model.state_dict())\n","        # lub do pliku\n","        # checkpoint(model, \"best_model.pth\")\n","    elif epoch - best_epoch > early_stop_thresh:\n","            print(\"Early stopped training at epoch %d\" % epoch)\n","            break  # terminate the training loop"],"metadata":{"id":"qaGpFAbnT1zN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Loss: %.2f\" % best_loss)\n","plt.plot(history)\n","plt.show()"],"metadata":{"id":"9oxRKBxQFGGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wczytanie modelu\n","model.load_state_dict(best_weights)\n","# lub z pliku\n","# resume(model, \"best_model.pth\")"],"metadata":{"id":"zbtd-i6j2p5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model(torch.tensor(x, dtype = torch.float32).to(device))"],"metadata":{"id":"Ayrgo0PfVdHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = go.Figure(data=[go.Scatter3d(x=x[:,0], y=x[:,1], z=x[:,2],\n","                                   mode='markers',marker = dict(\n","                                   color = predictions.detach().numpy().reshape(len(x)),\n","                                   colorbar=dict(\n","                                       title = 'y_hat'\n","                                   ),\n","                                    colorscale=\"Viridis\")\n","                                   )])\n","fig.show()"],"metadata":{"id":"vOV1Bi5SVD0w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataloader + SummaryWriter"],"metadata":{"id":"bh0SXdFi5Rfg"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, random_split, default_collate\n","from torch.utils.tensorboard import SummaryWriter"],"metadata":{"id":"Tsvvf6cF3ors"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n","y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1).to(device)"],"metadata":{"id":"ufDqIHb769RV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainset, testset = random_split(TensorDataset(x_tensor, y_tensor), [0.8, 0.2])"],"metadata":{"id":"1tV6-x2Q3uYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tworzymy loadery do zbiorów - zadbają o batch training\n","\n","train_loader = DataLoader(trainset, shuffle=True, batch_size=32)\n","test_loader = DataLoader(testset, shuffle=False, batch_size=32)"],"metadata":{"id":"Ukcl0nC_5Tz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wykorzystanie tensorboard do monitorowania procesu uczenia modelu\n","writer = SummaryWriter(filename_suffix='Initial training loop')"],"metadata":{"id":"Zd4JM3xpuch9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer, epoch, writer = None):\n","  model.train()\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","  train_loss, correct = 0, 0\n","  running_size = 0\n","  running_batches = 0\n","  for X_batch, y_batch in (pbar := tqdm(dataloader, desc = 'Train: ')):\n","    y_pred = model(X_batch)\n","    loss = loss_fn(y_pred, y_batch)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    train_loss += loss.item()\n","    correct += (((y_pred>0.5)*1) == y_batch).type(torch.float).sum().item()\n","    running_size += len(y_batch)\n","    running_batches +=1\n","    pbar.set_postfix_str(f\"loss: {float(train_loss/running_batches)}, acc: {correct/running_size}, [{running_size}/{size}]\")\n","\n","  train_loss /= num_batches\n","  correct /= size\n","\n","  if writer is not None: # Dodajemy skalary do tensorboard\n","    writer.add_scalar(\"Loss\", train_loss, epoch+1)\n","    writer.add_scalar(\"Accuracy\", correct, epoch+1)\n","    for name, weight in model.named_parameters():\n","      writer.add_histogram(name,weight, epoch)\n","      writer.add_histogram(f'{name}.grad',weight.grad, epoch)\n","  return train_loss, correct\n","\n","def test_loop(dataloader, model, loss_fn, epoch, writer = None):\n","  model.eval()\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","  test_loss, correct = 0, 0\n","  running_size = 0\n","  running_batches = 0\n","\n","  for X_batch, y_batch in (pbar := tqdm(dataloader, desc = 'Validation: ')):\n","    y_pred = model(X_batch)\n","    loss = loss_fn(y_pred, y_batch)\n","    test_loss += loss.item()\n","    correct += (((y_pred>0.5)*1) == y_batch).type(torch.float).sum().item()\n","    running_size += len(y_batch)\n","    running_batches +=1\n","    pbar.set_postfix_str(f\"loss: {float(test_loss/running_batches)}, acc: {correct/running_size}, [{running_size}/{size}]\")\n","\n","  test_loss /= num_batches\n","  correct /= size\n","\n","  if writer is not None:\n","    writer.add_scalar(\"Val Loss\", test_loss, epoch+1)\n","    writer.add_scalar(\"Val Accuracy\", correct, epoch+1)\n","  return test_loss, correct\n","\n","\n","class EarlyStopping:\n","  def __init__(self, tolerance=10, min_delta=0):\n","\n","    self.tolerance = tolerance\n","    self.min_delta = min_delta\n","    self.counter = 0\n","    self.best_weights = None\n","    self.best_loss = np.inf\n","\n","  def __call__(self, validation_loss, model):\n","    self._update_best_model_(validation_loss)\n","    if (self.best_loss - validation_loss) < self.min_delta:\n","      self.counter +=1\n","      if self.counter >= self.tolerance:  \n","          return True\n","    else:\n","      self.counter = 0\n","      self.best_loss = validation_loss\n","    return False\n","\n","  def _update_best_model_(self, validation_loss):\n","    if validation_loss < self.best_loss:\n","      self.best_weights = copy.deepcopy(model.state_dict())\n"],"metadata":{"id":"J5KoNEf_5cni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["early_stopping = EarlyStopping(tolerance=10, min_delta=0.05)"],"metadata":{"id":"Y70KmCLuHwms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","  def __init__(self, n_dims, n_class):\n","      super(NeuralNetwork, self).__init__()\n","      self.seq = nn.Sequential(\n","          nn.Linear(n_dims, 512),\n","          nn.ReLU(),\n","          nn.Linear(512, 512),\n","          nn.ReLU(),\n","          nn.Linear(512, n_class)\n","      )\n","\n","  def forward(self, x):\n","      logits = self.seq(x)\n","      return logits"],"metadata":{"id":"4NHYCZ7sTkCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = NeuralNetwork(3, 1)\n","print(model)"],"metadata":{"id":"5aI7um13dVAo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 1e-2"],"metadata":{"id":"F2bSMqkddVhD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_fn = nn.BCEWithLogitsLoss() # Zauważ, że model nie jest zakończony funkcją Sigmoid"],"metadata":{"id":"KHFu2vuidVhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"metadata":{"id":"QijP8g_1dVhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dodanie grafu obliczeniowego do tensorboard, w przypadku sequential nie wygląda to porywająco\n","\n","x_data, labels = next(iter(train_loader))\n","writer.add_graph(model, x_data)"],"metadata":{"id":"1_LzFaDbuI25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_losses = []\n","train_acc = []\n","val_losses  = []\n","val_acc = []\n","for epoch in range(n_epochs):\n","    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n","    train_loss, train_correct = train_loop(train_loader, model, loss_fn, optimizer, epoch, writer)\n","    train_losses.append(train_loss), train_acc.append(train_correct)\n","    test_loss, test_correct = test_loop(test_loader, model, loss_fn, epoch, writer)\n","    val_losses.append(test_loss), val_acc.append(test_correct)\n","    if early_stopping(validation_loss = test_loss, model = model):\n","      print(f'\\n-------------------------------\\nEarly stopped at epoch {epoch+1}')\n","      model.load_state_dict(early_stopping.best_weights)\n","\n","      break\n","writer.close()"],"metadata":{"id":"ncaI6rrfAXQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["writer.log_dir"],"metadata":{"id":"1pokzoD-vx9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"haC1Sb61wN26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir runs/Apr11_19-14-56_8312cedac1c4"],"metadata":{"id":"NJCwvJcBv2Zd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(train_losses)\n","plt.plot(val_losses)\n","plt.show()"],"metadata":{"id":"0MMOaR4viqIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(train_acc)\n","plt.plot(val_acc)\n","plt.show()"],"metadata":{"id":"d5dAiHsAjUKC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFo1v7MWc93l"},"source":["# Zadanie - predykcja popularności uworów muzycznych"]},{"cell_type":"markdown","metadata":{"id":"z76zo6CiYX5v"},"source":["#### Wgrywanie danych, EDA"]},{"cell_type":"markdown","metadata":{"id":"xMk5J2_s0yvN"},"source":["Na zajęciach przeanalizujemy zbiór danych dostępny na <a href=\"https://www.kaggle.com/priyang/health-insurance-cost-prediction-using-ml\">kaggle</a>. Obserwacje dotyczą wybranych opłat za ubezpieczenie zdrowotne na rynku USA. Celem modeli będzie ceny ubezpieczenia w zależności od wartości parametrów opisujących daną osobę."]},{"cell_type":"code","metadata":{"id":"FUJEb605HqSw"},"source":["import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RtdFANagdxJL"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t2rkOTj4ytTk"},"source":["Proszę:\n","<li>Utworzyć folder \"AI_datasets\" w lokalizacji \"Mój Dysk\"</li>\n","<li>Dodać plik 'insurance.csv' z moodle.</li>"]},{"cell_type":"code","metadata":{"id":"cVccqOaCebAt"},"source":["%cd /content/gdrive/My Drive/AI_datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5A4_6-jyAU6"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUuVdZ4vHf8h"},"source":["data = pd.read_csv(\"song_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfSX8b2eth42"},"source":["data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.duplicated(subset='song_name').sum()"],"metadata":{"id":"-g4utGUhmA1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Co powinniśmy teraz zrobić?"],"metadata":{"id":"Qi7lwpBimLQ7"}},{"cell_type":"code","source":["data.drop_duplicates(inplace = True)"],"metadata":{"id":"cZaUabS0mhaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SW72bh9BHnZS"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4pLuf2MJlxA"},"source":["data.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HS4dHbiQLdPS"},"source":["sns.pairplot(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corr = data.corr(method = 'spearman')\n","\n","# Generate a mask for the upper triangle\n","mask = np.triu(np.ones_like(corr, dtype=bool))\n","\n","# Set up the matplotlib figure\n","f, ax = plt.subplots(figsize=(11, 9))\n","\n","# Generate a custom diverging colormap\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","\n","# Draw the heatmap with the mask and correct aspect ratio\n","sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n","            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)"],"metadata":{"id":"CECquy5H8IXR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analiza zmiennej zależnej"],"metadata":{"id":"fq2SBdLUljT1"}},{"cell_type":"code","source":["y_col = \"song_popularity\""],"metadata":{"id":"pwABjli3lmA-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPKENBgSXsDo"},"source":["import scipy.stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0B7I3WuOl_0"},"source":["sns.displot(data, x=y_col, kind=\"kde\", bw_adjust=.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"257cQ9vVSIKs"},"source":["upper_bound = (data[y_col].quantile(0.75)+1.5*scipy.stats.iqr(data[y_col]))\n","lower_bound = (data[y_col].quantile(0.25)-1.5*scipy.stats.iqr(data[y_col]))\n","print(upper_bound,lower_bound)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HLD60gra4VQ"},"source":["sns.displot(data.loc[data[y_col]>2], x=y_col, kind=\"kde\", bw_adjust=.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4pboTuhbfIs"},"source":["data = data.loc[data[y_col]>2].copy()\n","data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analiza zmiennych zależnych"],"metadata":{"id":"EqaR0wbTorB_"}},{"cell_type":"markdown","source":["TODO - przeanalizuj wszystkie zmienne pod kątem ich wpływu na zmienną niezależną, rozkładu oraz wartości skrajnych."],"metadata":{"id":"wAabgTMUoyIM"}},{"cell_type":"markdown","metadata":{"id":"DrJbIRAFq0BL"},"source":["## Normalizacja i one hot encoding"]},{"cell_type":"code","source":["data.drop(columns = 'song_name', inplace = True)"],"metadata":{"id":"77l3KK8RpSBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trLsT1jOj8iz"},"source":["from sklearn.preprocessing import MinMaxScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scale_factors = {'min': data[y_col].min() , 'max' : data[y_col].max()}"],"metadata":{"id":"vo9pkfdhIGxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scale_factors"],"metadata":{"id":"4TeEqZBCIb-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhesfdZrJyH4"},"source":["scaler = MinMaxScaler() \n","scaled_values = scaler.fit_transform(data) \n","data = pd.DataFrame(scaled_values, columns = data.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpB--AL9lEfe"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.describe()"],"metadata":{"id":"cIXO5KpVB0by"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = data[y_col].copy()\n","x = data.drop(columns = y_col).copy()"],"metadata":{"id":"TWb85jhTqTVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phQvTGW-ZEN9"},"source":["## Predykcja popularności"]},{"cell_type":"markdown","source":["## Wykorzystanie tensorboard do wyboru najlepszych parametrów sieci w pytorch"],"metadata":{"id":"O4j_isaso3wh"}},{"cell_type":"code","source":["from itertools import product"],"metadata":{"id":"f4cx_P_4o3wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tworzymy przestrzeń parametrów, które będą sprawdzane w treningu sieci\n","parameters = dict(\n","    lr = [0.01, 0.001],\n","    use_RMSE = [True,False]\n",")\n","\n","param_values = [v for v in parameters.values()]\n","\n","for lr,use_RMSE in product(*param_values):\n","    print(lr, use_RMSE)"],"metadata":{"id":"mnLSnDJ3o3wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F"],"metadata":{"id":"BfSph7Fwpv_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Zauważcie, że nie korzystamy z modułu Sequential - sprawdź jak wpłynie na widok grafu obliczniowego\n","\n","class RegNet_4(nn.Module):\n","  def __init__(self, n_dims):\n","      super(RegNet_4, self).__init__()\n","      self.fc1 = nn.Linear(n_dims, 32)\n","      self.bn1 = nn.BatchNorm1d(32)\n","      self.fc2 = nn.Linear(32, 64)\n","      self.bn2 = nn.BatchNorm1d(64)\n","      self.fc3 = nn.Linear(64, 32)\n","      self.bn3 = nn.BatchNorm1d(32)\n","      self.fc4 = nn.Linear(32, 8)\n","      self.out =  nn.Linear(8, 1)\n","\n","  def forward(self, x):\n","      x = F.relu(self.bn1(self.fc1(x)))\n","      x = F.relu(self.bn2(self.fc2(x)))\n","      x = F.relu(self.bn3(self.fc3(x)))\n","      x = F.relu(self.fc4(x))\n","      x = self.out(x)\n","      return x"],"metadata":{"id":"sKz8FwXtpDrc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Do poczytania - co to batch normalization ? "],"metadata":{"id":"8SEvHDMqeTGV"}},{"cell_type":"code","source":["x_tensor = torch.tensor(x.values, dtype=torch.float32).to(device)\n","y_tensor = torch.tensor(y.values, dtype=torch.float32).reshape(-1, 1).to(device)"],"metadata":{"id":"euMEI_FZo3wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainset, testset = random_split(TensorDataset(x_tensor, y_tensor), [0.8, 0.2])"],"metadata":{"id":"ACwj-BLto3wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(trainset, shuffle=True, batch_size=32)\n","test_loader = DataLoader(testset, shuffle=False, batch_size=32)"],"metadata":{"id":"5_gFNZYTo3wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter"],"metadata":{"id":"3pONQvamo3wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer, epoch, lr, use_RMSE, writer = None):\n","  model.train()\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","  train_loss, correct = 0, 0\n","  running_size = 0\n","  running_batches = 0\n","  for X_batch, y_batch in (pbar := tqdm(dataloader, desc = 'Train: ')):\n","    y_pred = model(X_batch)\n","    loss = loss_fn(y_pred, y_batch)\n","    if use_RMSE:\n","      loss = torch.sqrt(loss)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    train_loss += loss.item()\n","    running_size += len(y_batch)\n","    running_batches +=1\n","    pbar.set_postfix_str(f\"loss: {float(train_loss/running_batches)}, [{running_size}/{size}]\")\n","\n","  train_loss /= num_batches\n","\n","  if writer is not None:\n","    if use_RMSE:\n","      writer.add_scalar(\"Loss\", train_loss, epoch+1)\n","    else:\n","      writer.add_scalar(\"Loss\", np.sqrt(train_loss), epoch+1)\n","\n","\n","def test_loop(dataloader, model, loss_fn, epoch, lr, use_RMSE, writer = None):\n","  model.eval()\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","  test_loss, correct = 0, 0\n","  running_size = 0\n","  running_batches = 0\n","\n","  for X_batch, y_batch in (pbar := tqdm(dataloader, desc = 'Validation: ')):\n","    y_pred = model(X_batch)\n","    loss = loss_fn(y_pred, y_batch)\n","    if use_RMSE:\n","      loss = torch.sqrt(loss)\n","    test_loss += loss.item()\n","    running_size += len(y_batch)\n","    running_batches +=1\n","    pbar.set_postfix_str(f\"loss: {float(test_loss/running_batches)}, [{running_size}/{size}]\")\n","\n","  test_loss /= num_batches\n","\n","  if writer is not None:\n","    if use_RMSE:\n","      writer.add_scalar(\"Val Loss\", test_loss, epoch+1)\n","    else:\n","      writer.add_scalar(\"Val Loss\", np.sqrt(test_loss), epoch+1)\n","  return test_loss\n","\n","class EarlyStopping:\n","  def __init__(self, tolerance=10, min_delta=0):\n","\n","    self.tolerance = tolerance\n","    self.min_delta = min_delta\n","    self.counter = 0\n","    self.best_weights = None\n","    self.best_loss = np.inf\n","\n","  def __call__(self, validation_loss, model):\n","    self._update_best_model_(validation_loss)\n","    if (self.best_loss - validation_loss) < self.min_delta:\n","      self.counter +=1\n","      if self.counter >= self.tolerance:  \n","          return True\n","    else:\n","      self.counter = 0\n","      self.best_loss = validation_loss\n","    return False\n","\n","  def _update_best_model_(self, validation_loss):\n","    if validation_loss < self.best_loss:\n","      self.best_weights = copy.deepcopy(model.state_dict())\n"],"metadata":{"id":"Q4HsyyHQo3wj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epochs = 100\n","loss_fn = nn.MSELoss()"],"metadata":{"id":"TYOYlDZxu4Kr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape[1]"],"metadata":{"id":"3qabrB6mwLlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_states = []\n","for run_id, (lr,use_RMSE) in enumerate(product(*param_values)):\n","    print(\"run id:\", run_id + 1)\n","    model = RegNet_4(x.shape[1]).to(device)\n","\n","    # dodanie grafu obliczeniowego do tensorboard, w przypadku sequential nie wygląda to porywająco\n","\n","    x_data, labels = next(iter(train_loader))\n","    writer.add_graph(model, x_data)\n","\n","\n","    early_stopping = EarlyStopping(tolerance=10, min_delta=0.01)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    criterion = torch.nn.MSELoss()\n","    comment = f' lr = {lr} use_RMSE = {use_RMSE}'\n","    writer = SummaryWriter(comment=comment)\n","    for epoch in range(n_epochs):\n","      print(f\"Epoch {epoch+1}\\n-------------------------------\")\n","      train_loop(train_loader, model, loss_fn, optimizer, epoch, lr, use_RMSE, writer)\n","      test_loss = test_loop(test_loader, model, loss_fn, epoch, lr, use_RMSE, writer)\n","      if early_stopping(validation_loss = test_loss, model = model):\n","        print(f'\\n-------------------------------\\nEarly stopped at epoch {epoch+1}')\n","        model_states.append(early_stopping.best_weights)\n","        if use_RMSE:\n","          writer.add_hparams(\n","                  {\"lr\": lr, \"use_RMSE\": use_RMSE},\n","                  {\n","                      \"loss\": test_loss,\n","                  },\n","              )\n","        else:\n","          writer.add_hparams(\n","                  {\"lr\": lr, \"use_RMSE\": use_RMSE},\n","                  {\n","                      \"loss\": np.sqrt(test_loss),\n","                  },\n","              )\n","        break\n","writer.close()"],"metadata":{"id":"UUWW6u0Ho3wj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["writer.log_dir"],"metadata":{"id":"QC1YKtVrzVXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! rm -r runs"],"metadata":{"id":"rwM-0Alu0P-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir runs"],"metadata":{"id":"D9_KdiZjynj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kill $( lsof -i:6006 -t )"],"metadata":{"id":"-TVhtDgtzg4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RegNet_4(x.shape[1]).to(device)\n","model.load_state_dict(model_states[1])"],"metadata":{"id":"QB1BEwngdgja"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# XAI"],"metadata":{"id":"i-Pg2v58qAba"}},{"cell_type":"markdown","source":["W przypadku pytorch dysponujemy dedykowaną biblioteką do XAI - Captum."],"metadata":{"id":"3mj4iT-hecXh"}},{"cell_type":"code","source":["!pip install captum"],"metadata":{"id":"Ab8l3evVfUix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train = trainset.dataset.tensors[0][trainset.indices[:100]]\n","x_test = testset.dataset.tensors[0][testset.indices[:100]]\n"],"metadata":{"id":"17dfNl_w4-cR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_names = x.columns"],"metadata":{"id":"XoXNcnFM6gLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# imports from captum library\n","from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients, IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation, NeuronConductance"],"metadata":{"id":"wOnyQ5kNhwrt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analiza modelu"],"metadata":{"id":"stb1LV-ufJ-i"}},{"cell_type":"markdown","source":["Captum oferuje różne metody do sprawdzenia zachowania modelu oparte o gradienty, wartości SHAP czy wykorzystanie szumu. Porównajmy ich zachowanie:"],"metadata":{"id":"U25xk1LhekJG"}},{"cell_type":"code","source":["ig = IntegratedGradients(model)\n","ig_nt = NoiseTunnel(ig)\n","dl = DeepLift(model)\n","gs = GradientShap(model)\n","fa = FeatureAblation(model)\n","\n","ig_attr_test = ig.attribute(x_test, n_steps=50)\n","ig_nt_attr_test = ig_nt.attribute(x_test)\n","dl_attr_test = dl.attribute(x_test)\n","gs_attr_test = gs.attribute(x_test, x_train)\n","fa_attr_test = fa.attribute(x_test)"],"metadata":{"id":"9L6Ix-HxhpzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare attributions for visualization\n","\n","x_axis_data = np.arange(x_test.shape[1])\n","x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n","\n","ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\n","ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n","\n","ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n","ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n","\n","dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)\n","dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)\n","\n","gs_attr_test_sum = gs_attr_test.detach().numpy().sum(0)\n","gs_attr_test_norm_sum = gs_attr_test_sum / np.linalg.norm(gs_attr_test_sum, ord=1)\n","\n","fa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\n","fa_attr_test_norm_sum = fa_attr_test_sum / np.linalg.norm(fa_attr_test_sum, ord=1)\n","\n","lin_weight = model.fc1.weight[0].detach().numpy()\n","y_axis_lin_weight = lin_weight / np.linalg.norm(lin_weight, ord=1)\n","\n","width = 0.14\n","legends = ['Int Grads', 'Int Grads w/SmoothGrad','DeepLift', 'GradientSHAP', 'Feature Ablation', 'Weights']\n","\n","plt.figure(figsize=(20, 10))\n","\n","ax = plt.subplot()\n","ax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\n","ax.set_ylabel('Attributions')\n","\n","FONT_SIZE = 16\n","plt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\n","plt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\n","plt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\n","plt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n","\n","ax.bar(x_axis_data, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\n","ax.bar(x_axis_data + width, ig_nt_attr_test_norm_sum, width, align='center', alpha=0.7, color='#A90000')\n","ax.bar(x_axis_data + 2 * width, dl_attr_test_norm_sum, width, align='center', alpha=0.6, color='#34b8e0')\n","ax.bar(x_axis_data + 3 * width, gs_attr_test_norm_sum, width, align='center',  alpha=0.8, color='#4260f5')\n","ax.bar(x_axis_data + 4 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\n","ax.bar(x_axis_data + 5 * width, y_axis_lin_weight, width, align='center', alpha=1.0, color='grey')\n","ax.autoscale_view()\n","plt.tight_layout()\n","\n","ax.set_xticks(x_axis_data + 0.5)\n","ax.set_xticklabels(x_axis_data_labels)\n","\n","plt.legend(legends, loc=3)\n","plt.show()"],"metadata":{"id":"JqMyuApfhrBB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Zauważ, że metody wydają się nie być ze sobą zgodne."],"metadata":{"id":"MY-dxzx3e84N"}},{"cell_type":"code","source":["def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n","    print(title)\n","    for i in range(len(feature_names)):\n","        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n","    x_pos = (np.arange(len(feature_names)))\n","    if plot:\n","        plt.figure(figsize=(18,6))\n","        FONT_SIZE = 10\n","        plt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\n","        plt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\n","        plt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\n","        plt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n","        plt.bar(x_pos, importances, align='center')\n","        plt.xticks(x_pos, feature_names, wrap=True)\n","        plt.xlabel(axis_title)\n","        plt.title(title)"],"metadata":{"id":"U9YhC-f5gWx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_importances(feature_names, np.mean(ig_attr_test.detach().numpy(), axis=0))"],"metadata":{"id":"-2HBCWcb8qTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analiza wybranej warstwy"],"metadata":{"id":"kVocaeDqfFIs"}},{"cell_type":"code","source":["cond = LayerConductance(model, model.fc1)"],"metadata":{"id":"K3iAjgGIgqFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cond_vals = cond.attribute(x_test)\n","cond_vals = cond_vals.detach().numpy()"],"metadata":{"id":"ri2EoKR0gqDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_importances(range(32),np.mean(cond_vals, axis=0),title=\"Average Neuron Importances\", axis_title=\"Neurons\")"],"metadata":{"id":"vRF1nZIpgqBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cond = LayerConductance(model, model.fc4)"],"metadata":{"id":"DC7zy4A--ENO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cond_vals = cond.attribute(x_test)\n","cond_vals = cond_vals.detach().numpy()"],"metadata":{"id":"dTiovMGf-ENP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_importances(range(8),np.mean(cond_vals, axis=0),title=\"Average Neuron Importances\", axis_title=\"Neurons\")"],"metadata":{"id":"umR_8dC--ENP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(cond_vals[:,1], 100);\n","plt.title(\"Neuron 1 Distribution\")\n","plt.figure()\n","plt.hist(cond_vals[:,2], 100);\n","plt.title(\"Neuron 2 Distribution\");"],"metadata":{"id":"QZDrthKegp-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(cond_vals[:,0], 100);\n","plt.title(\"Neuron 0 Distribution\")\n","plt.figure()\n","plt.hist(cond_vals[:,4], 100);\n","plt.title(\"Neuron 4 Distribution\");"],"metadata":{"id":"pJxSTs27hB7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analiza wybranego neuronu"],"metadata":{"id":"lJgrAfftfO0d"}},{"cell_type":"code","source":["neuron_cond = NeuronConductance(model, model.fc4)"],"metadata":{"id":"i3eW36AmhB28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neuron_cond_vals_4 = neuron_cond.attribute(x_test, neuron_selector=4)"],"metadata":{"id":"gln1obiHhB0v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_importances(feature_names, neuron_cond_vals_4.mean(dim=0).detach().numpy(), title=\"Average Feature Importances for Neuron 4\")\n"],"metadata":{"id":"rFxazYU2hQmY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Zadanie - wytrenuj model, w którym więcej neuronów będzie decydowało o predykcji."],"metadata":{"id":"L2NcNjyMIQB8"}},{"cell_type":"code","source":[],"metadata":{"id":"JDuIgGLSIYLQ"},"execution_count":null,"outputs":[]}]}